{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    # Fix softmax when using batch size 1 the dimension of deno changes\n",
    "    if(len(train_data) == 1):\n",
    "        deno = np.sum(num,axis=0)\n",
    "    else:\n",
    "        deno = np.sum(num,axis=1)\n",
    "        deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    for i in range(len(target)):\n",
    "        if(predicted[i] == target[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[predicted[i]][target[i]] =confusion_mat[predicted[i]][target[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[tuple_data[1][i]])\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('./models/logisticModel', weights)\n",
    "#np.loadtxt('./models/logisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "train_data = np.append(training_data[0],validation_data[0],axis=0)\n",
    "train_target = np.append(training_data[1],validation_data[1])\n",
    "test_target = test_data[1]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "scaler.fit(test_data)\n",
    "processed_test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#rf = joblib.load(\"./models/randomForestModel.joblib\")\n",
    "#lr = np.loadtxt(\"./models/logisticModel\")\n",
    "#nn = joblib.load(\"./models/DNN_lowHidden.joblib\")\n",
    "#svm model pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve order since LR is hardcoded and methods like rf depend on predic_proba :3\n",
    "classifiers = [lr,nn,rf]\n",
    "weights = [1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(classifiers,processed_test_data,test_data,weights):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = np.multiply(softmax(classifiers[0],processed_test_data),weights[0])\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = np.multiply(classifiers[1].predict(test_data,verbose=True),weights[1])\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = np.multiply(classifiers[2].predict_proba(processed_test_data),weights[2])\n",
    "    sumProb = (np.transpose(lr_pred)+rf_pred+nn_pred)\n",
    "    wtAvg = np.divide(sumProb,len(classifiers))\n",
    "    return np.argmax(wtAvg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 2] the accuracy is: 0.1006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brute Force to find best weights\n",
    "def findWeights(classifiers,processed_test_data,test_data,test_target,weights)\n",
    "    tracker = []\n",
    "    for weight1 in range(1,4):\n",
    "        for weight2 in range(1,4):\n",
    "            for weight3 in range(1,4):\n",
    "                if(not(weight1 == weight2 == weight3)):\n",
    "                    weights=[weight1,weight2,weight3]\n",
    "                    predicted = ensemble(classifiers,processed_test_data,test_data,weights)\n",
    "                    acc,_ = accuracy(predicted,test_target)\n",
    "                    print()\n",
    "                    print(\"For weights \"+str(weights)+\" the accuracy is: \"+str(acc))\n",
    "                    tracker.append([weights,acc])\n",
    "                    print()\n",
    "    max_acc = 0\n",
    "    tracker = np.asarray(tracker)\n",
    "    for i,sample in enumerate(tracker):\n",
    "    if(max_acc<tracker[i][1]):\n",
    "        max_acc=tracker[i][1]\n",
    "        opt_weight = tracker[1][0]\n",
    "    return opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "Working on Random Forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9832,      0     1     2    3    4    5    6     7    8    9\n",
       " 0  972     0     2    1    0    2    4     1    0    1\n",
       " 1    1  1129     0    0    0    0    2     1    0    1\n",
       " 2    0     1  1011    1    2    0    0     6    2    0\n",
       " 3    1     2     1  981    1    3    1     2    4    2\n",
       " 4    0     0     1    0  962    0    2     0    3    7\n",
       " 5    0     1     1   13    0  881    2     0    2    5\n",
       " 6    2     0     1    0    3    2  943     0    1    1\n",
       " 7    1     2    11    8    2    1    0  1016    5    5\n",
       " 8    3     0     3    3    1    1    4     2  953    3\n",
       " 9    0     0     1    3   11    2    0     0    4  984)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_test_data,test_data,weights)\n",
    "accuracy(predicted,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '../USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_USPS = one_hot_vect((1,USPSTar),np.unique(USPSTar))\n",
    "scaler.fit(USPSMat)\n",
    "processed_USPSDat = scaler.transform(USPSMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[3,8,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 2s 79us/step\n",
      "Working on Random Forest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5059252962648132,      0    1     2     3     4     5     6    7    8    9\n",
       " 0  836   71    54    13    32    54   116   48  150   37\n",
       " 1    4  575     4     4    71     3    15  234   12  104\n",
       " 2   66  144  1548    71    30    58   201  215   90   54\n",
       " 3   93  144    86  1465    23    68    44  566  328  259\n",
       " 4  186  321    13     3  1002     5    38   31   69  141\n",
       " 5  154  131    86   347   129  1647   226   50  362   93\n",
       " 6  112   91    79     6    34    15  1201    6   51    6\n",
       " 7  150  349    57    21   320    55    40  690   87  659\n",
       " 8   90   92    66    61   301    82    69  148  824  317\n",
       " 9  309   82     6     9    58    13    50   12   27  330)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_USPSDat,[USPSMat],weights)\n",
    "accuracy(predicted,USPSTar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
