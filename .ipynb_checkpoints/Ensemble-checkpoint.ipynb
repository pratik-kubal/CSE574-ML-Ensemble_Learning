{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    # Fix softmax when using batch size 1 the dimension of deno changes\n",
    "    if(len(train_data) == 1):\n",
    "        deno = np.sum(num,axis=0)\n",
    "    else:\n",
    "        deno = np.sum(num,axis=1)\n",
    "        deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    for i in range(len(target)):\n",
    "        if(predicted[i] == target[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[target[i]][predicted[i]] =confusion_mat[target[i]][predicted[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[tuple_data[1][i]])\n",
    "    return one_hot_encoded\n",
    "\n",
    "def more_metrics(conf_mat):\n",
    "    true_positives = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        true_positives += conf_mat.iloc[i,i]\n",
    "    conf_mat = np.matrix(conf_mat)\n",
    "    tp_fp = np.array(np.sum(conf_mat,axis=1)).ravel()\n",
    "    relevant_elements = np.array(np.sum(conf_mat,axis=0)).ravel()\n",
    "    for i in range(len(conf_mat)):\n",
    "        precision.append(conf_mat[i,i]/tp_fp[i])\n",
    "        recall.append(conf_mat[i,i]/relevant_elements[i])\n",
    "    return true_positives,precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('./models/logisticModel', weights)\n",
    "#np.loadtxt('./models/logisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "train_data = np.append(training_data[0],validation_data[0],axis=0)\n",
    "train_target = np.append(training_data[1],validation_data[1])\n",
    "test_target = test_data[1]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "scaler.fit(test_data)\n",
    "processed_test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator SVC from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = joblib.load(\"./models/randomForestModel.joblib\")\n",
    "lr = np.loadtxt(\"./models/logisticModel\")\n",
    "nn = joblib.load(\"./models/DNN_lowHidden.joblib\")\n",
    "svm = joblib.load(\"./models/SVMpdfModel3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve order since LR is hardcoded and methods like rf depend on predic_proba :3\n",
    "classifiers = [lr,nn,rf,svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(classifiers,processed_test_data,test_data,weights,svmInclude=False):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = np.multiply(softmax(classifiers[0],processed_test_data),weights[0])\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = np.multiply(classifiers[1].predict(test_data,verbose=True),weights[1])\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = np.multiply(classifiers[2].predict_proba(processed_test_data),weights[2])\n",
    "    sumProb = (np.transpose(lr_pred)+rf_pred+nn_pred)\n",
    "    wtAvg = np.divide(sumProb,3)\n",
    "    if(svmInclude):\n",
    "        print(\"Working on SVM\")\n",
    "        svm_pred = np.multiply(classifiers[3].predict_proba(processed_test_data),weights[3])\n",
    "        sumProb = sumProb+svm_pred\n",
    "        wtAvg = np.divide(sumProb,4)\n",
    "    return np.argmax(wtAvg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute Force to find best weights\n",
    "def findWeights(classifiers,processed_test_data,test_data,test_target):\n",
    "    tracker = []\n",
    "    for weight1 in range(1,4):\n",
    "        for weight2 in range(1,4):\n",
    "            for weight3 in range(1,4):\n",
    "                if(not(weight1 == weight2 == weight3)):\n",
    "                    weights=[weight1,weight2,weight3]\n",
    "                    predicted = ensemble(classifiers,processed_test_data,test_data,weights)\n",
    "                    acc,_ = accuracy(predicted,test_target)\n",
    "                    print()\n",
    "                    print(\"For weights \"+str(weights)+\" the accuracy is: \"+str(acc))\n",
    "                    tracker.append([weights,acc])\n",
    "                    print()\n",
    "    max_acc = 0\n",
    "    tracker = np.asarray(tracker)\n",
    "    for i,sample in enumerate(tracker):\n",
    "        if(max_acc<tracker[i][1]):\n",
    "            max_acc=tracker[i][1]\n",
    "            opt_weight = tracker[1][0]\n",
    "    return opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 2] the accuracy is: 0.9825\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 3] the accuracy is: 0.9832\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 1] the accuracy is: 0.9824\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 2] the accuracy is: 0.9823\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 3] the accuracy is: 0.9826\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 1] the accuracy is: 0.9824\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 2] the accuracy is: 0.9824\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 3] the accuracy is: 0.9823\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 1] the accuracy is: 0.9823\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Working on Random Forest\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-b7881dd0a1ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessed_test_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-85229814142e>\u001b[0m in \u001b[0;36mfindWeights\u001b[0;34m(classifiers, processed_test_data, test_data, test_target)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweight2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweight3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessed_test_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-1e63857c3574>\u001b[0m in \u001b[0;36mensemble\u001b[0;34m(classifiers, processed_test_data, test_data, weights, svmInclude)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnn_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working on Random Forest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msumProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrf_pred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwtAvg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumProb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    595\u001b[0m             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n\u001b[1;32m    596\u001b[0m                                             lock)\n\u001b[0;32m--> 597\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mcomplains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mplaced\u001b[0m \u001b[0mthere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_weight = findWeights(classifiers,processed_test_data,test_data,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Working on Random Forest\n"
     ]
    }
   ],
   "source": [
    "'''It would be crazy to brute force with SVM so now iterate over possible three values for SVM to find the \n",
    "maximum accuracy'''\n",
    "for weight in range(1,4):\n",
    "    working_wt = opt_weight.copy()\n",
    "    working_wt.append(weight)\n",
    "    predicted = ensemble(classifiers,processed_test_data,test_data,opt_weight,svmInclude=True)\n",
    "    print()\n",
    "    acc,_ = accuracy(predicted,test_target)\n",
    "    print(\"For weights \"+str(opt_weight)+\" And SVM \"+str(weight)+\" the accuracy is: \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for MNIST is: 0.9832\n",
      "The Confusion Matrix is: \n",
      "     0     1     2    3    4    5    6     7    8    9\n",
      "0  972     1     0    1    0    0    2     1    3    0\n",
      "1    0  1129     1    2    0    1    0     2    0    0\n",
      "2    2     0  1011    1    1    1    1    11    3    1\n",
      "3    1     0     1  981    0   13    0     8    3    3\n",
      "4    0     0     2    1  962    0    3     2    1   11\n",
      "5    2     0     0    3    0  881    2     1    1    2\n",
      "6    4     2     0    1    2    2  943     0    4    0\n",
      "7    1     1     6    2    0    0    0  1016    2    0\n",
      "8    0     0     2    4    3    2    1     5  953    4\n",
      "9    1     1     0    2    7    5    1     5    3  984\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  99.183673  98.880977\n",
      "1  99.471366  99.559083\n",
      "2  97.965116  98.826979\n",
      "3  97.128713  98.296593\n",
      "4  97.963340  98.666667\n",
      "5  98.766816  97.348066\n",
      "6  98.434238  98.950682\n",
      "7  98.832685  96.669838\n",
      "8  97.843943  97.944502\n",
      "9  97.522299  97.910448\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_test_data,test_data,opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,test_target)\n",
    "print(\"The Accuracy for MNIST is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '../USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_USPS = one_hot_vect((1,USPSTar),np.unique(USPSTar))\n",
    "scaler.fit(USPSMat)\n",
    "processed_USPSDat = scaler.transform(USPSMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 1s 73us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for USPS is: 0.5059252962648132\n",
      "The Confusion Matrix is: \n",
      "     0    1     2     3     4     5     6    7    8    9\n",
      "0  836   71    54    13    32    54   116   48  150   37\n",
      "1    4  575     4     4    71     3    15  234   12  104\n",
      "2   66  144  1548    71    30    58   201  215   90   54\n",
      "3   93  144    86  1465    23    68    44  566  328  259\n",
      "4  186  321    13     3  1002     5    38   31   69  141\n",
      "5  154  131    86   347   129  1647   226   50  362   93\n",
      "6  112   91    79     6    34    15  1201    6   51    6\n",
      "7  150  349    57    21   320    55    40  690   87  659\n",
      "8   90   92    66    61   301    82    69  148  824  317\n",
      "9  309   82     6     9    58    13    50   12   27  330\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  59.248760  41.800000\n",
      "1  56.042885  28.750000\n",
      "2  62.494954  77.438719\n",
      "3  47.626788  73.250000\n",
      "4  55.389718  50.100000\n",
      "5  51.069767  82.350000\n",
      "6  75.015615  60.050000\n",
      "7  28.418451  34.500000\n",
      "8  40.195122  41.200000\n",
      "9  36.830357  16.500000\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_USPSDat,[USPSMat],opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,USPSTar)\n",
    "print(\"The Accuracy for USPS is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemblePlus(classifiers,processed_test_data,test_data):\n",
    "    # Uses the predicted outputs of the labels\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = softmax(classifiers[0],processed_test_data)\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = classifiers[1].predict(test_data,verbose=True)\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = classifiers[2].predict_proba(processed_test_data)\n",
    "    print(\"Working on SVM Might take a while\")\n",
    "    if(len(np.shape(test_data))==3):\n",
    "        test_data = np.squeeze(test_data)\n",
    "    svm_pred = classifiers[3].predict_proba(test_data)\n",
    "    \n",
    "    featureVec = np.vstack([np.argmax(nn_pred,axis=1),np.argmax(rf_pred,axis=1),np.argmax(svm_pred,axis=1),np.argmax(lr_pred,axis=0)])\n",
    "    \n",
    "    return np.transpose(featureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemblePlusPlus(classifiers,processed_test_data,test_data):\n",
    "    # Uses the probabalities of the Predictions\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = softmax(classifiers[0],processed_test_data)\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = classifiers[1].predict(test_data,verbose=True)\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = classifiers[2].predict_proba(processed_test_data)\n",
    "    print(\"Working on SVM Might take a while\")\n",
    "    if(len(np.shape(test_data))==3):\n",
    "        test_data = np.squeeze(test_data)\n",
    "    svm_pred = classifiers[3].predict_proba(test_data)\n",
    "    \n",
    "    featureVec = np.hstack([nn_pred,rf_pred,svm_pred,np.transpose(lr_pred)])\n",
    "    \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 74us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#train_ensembleOutput =  ensemblePlusPlus(classifiers,processed_train_data,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superRF = RandomForestClassifier(n_estimators=1000,verbose=True)\n",
    "superRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superRF.fit(train_ensembleOutput,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#test_ensembleOutput =  ensemblePlusPlus(classifiers,processed_test_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 1s 65us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#test_ensembleOutput_USPS = ensemblePlusPlus(classifiers,processed_USPSDat,[USPSMat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for soft voting Random Forest for MNIST is: 0.9847\n",
      "The Confusion Matrix is: \n",
      "     0     1     2    3    4    5    6     7    8    9\n",
      "0  972     1     0    1    0    0    2     1    3    0\n",
      "1    0  1129     1    2    0    1    0     2    0    0\n",
      "2    3     0  1018    0    1    0    1     4    4    1\n",
      "3    1     0     1  986    0    8    0     7    3    4\n",
      "4    0     0     3    1  963    0    3     0    1   11\n",
      "5    2     0     0    4    0  880    1     1    1    3\n",
      "6    4     2     0    1    2    2  942     1    4    0\n",
      "7    1     1     6    2    0    0    0  1011    4    3\n",
      "8    0     0     2    4    2    2    1     4  956    3\n",
      "9    0     0     0    1    7    5    1     1    4  990\n",
      "The Precision & Recall is: \n",
      "      Recall  Precision\n",
      "0  99.183673  98.880977\n",
      "1  99.471366  99.646955\n",
      "2  98.643411  98.739088\n",
      "3  97.623762  98.403194\n",
      "4  98.065173  98.769231\n",
      "5  98.654709  97.995546\n",
      "6  98.329854  99.053628\n",
      "7  98.346304  97.965116\n",
      "8  98.151951  97.551020\n",
      "9  98.116947  97.536946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "acc,conf_mat = accuracy(superRF.predict(test_ensembleOutput),test_target)\n",
    "print(\"The Accuracy for soft voting Random Forest for MNIST is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Recall\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Precision\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy for soft voting Random Forest for USPS is: 0.5089754487724386\n",
      "The Confusion Matrix is: \n",
      "     0    1     2     3     4     5     6    7    8    9\n",
      "0  841    4    79    91   188   151   108  135   88  315\n",
      "1   72  590   153   147   314   121    86  345   95   77\n",
      "2   58    4  1576    73    15    80    75   50   62    6\n",
      "3   18    4    71  1501     3   313     6   16   59    9\n",
      "4   30   74    33    25  1017   128    33  302  299   59\n",
      "5   58    4    62    72     5  1641    15   52   77   14\n",
      "6  126   16   207    43    36   227  1199   36   60   50\n",
      "7   50  237   248   560    31    52     6  663  141   12\n",
      "8  152   10    94   333    62   380    49   75  817   28\n",
      "9   39  115    70   268   132    91     6  614  331  334\n",
      "The Precision & Recall is: \n",
      "     Recall  Precision\n",
      "0  42.05000  58.240997\n",
      "1  29.50000  55.765595\n",
      "2  78.83942  60.779020\n",
      "3  75.05000  48.217154\n",
      "4  50.85000  56.405990\n",
      "5  82.05000  51.538945\n",
      "6  59.95000  75.742262\n",
      "7  33.15000  28.977273\n",
      "8  40.85000  40.266141\n",
      "9  16.70000  36.946903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "acc,conf_mat = accuracy(superRF.predict(test_ensembleOutput_USPS),USPSTar)\n",
    "print(\"The Accuracy for soft voting Random Forest for USPS is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Recall\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Precision\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
