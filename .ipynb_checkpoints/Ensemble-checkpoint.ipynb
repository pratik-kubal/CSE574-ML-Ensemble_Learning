{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    # Fix softmax when using batch size 1 the dimension of deno changes\n",
    "    if(len(train_data) == 1):\n",
    "        deno = np.sum(num,axis=0)\n",
    "    else:\n",
    "        deno = np.sum(num,axis=1)\n",
    "        deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    for i in range(len(target)):\n",
    "        if(predicted[i] == target[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[target[i]][predicted[i]] =confusion_mat[target[i]][predicted[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[tuple_data[1][i]])\n",
    "    return one_hot_encoded\n",
    "\n",
    "def more_metrics(conf_mat):\n",
    "    true_positives = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        true_positives += conf_mat.iloc[i,i]\n",
    "    conf_mat = np.matrix(conf_mat)\n",
    "    tp_fp = np.array(np.sum(conf_mat,axis=1)).ravel()\n",
    "    relevant_elements = np.array(np.sum(conf_mat,axis=0)).ravel()\n",
    "    for i in range(len(conf_mat)):\n",
    "        precision.append(conf_mat[i,i]/tp_fp[i])\n",
    "        recall.append(conf_mat[i,i]/relevant_elements[i])\n",
    "    return true_positives,precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('./models/logisticModel', weights)\n",
    "#np.loadtxt('./models/logisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "train_data = np.append(training_data[0],validation_data[0],axis=0)\n",
    "train_target = np.append(training_data[1],validation_data[1])\n",
    "test_target = test_data[1]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "scaler.fit(test_data)\n",
    "processed_test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator SVC from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = joblib.load(\"./models/randomForestModel.joblib\")\n",
    "lr = np.loadtxt(\"./models/logisticModel\")\n",
    "nn = joblib.load(\"./models/DNN_lowHidden.joblib\")\n",
    "svm = joblib.load(\"./models/SVMpdfModel3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve order since LR is hardcoded and methods like rf depend on predic_proba :3\n",
    "classifiers = [lr,nn,rf,svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(classifiers,processed_test_data,test_data,weights):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = np.multiply(softmax(classifiers[0],processed_test_data),weights[0])\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = np.multiply(classifiers[1].predict(test_data,verbose=True),weights[1])\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = np.multiply(classifiers[2].predict_proba(processed_test_data),weights[2])\n",
    "    sumProb = (np.transpose(lr_pred)+rf_pred+nn_pred)\n",
    "    wtAvg = np.divide(sumProb,len(classifiers))\n",
    "    return np.argmax(wtAvg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 2] the accuracy is: 0.1006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brute Force to find best weights\n",
    "def findWeights(classifiers,processed_test_data,test_data,test_target,weights)\n",
    "    tracker = []\n",
    "    for weight1 in range(1,4):\n",
    "        for weight2 in range(1,4):\n",
    "            for weight3 in range(1,4):\n",
    "                if(not(weight1 == weight2 == weight3)):\n",
    "                    weights=[weight1,weight2,weight3]\n",
    "                    predicted = ensemble(classifiers,processed_test_data,test_data,weights)\n",
    "                    acc,_ = accuracy(predicted,test_target)\n",
    "                    print()\n",
    "                    print(\"For weights \"+str(weights)+\" the accuracy is: \"+str(acc))\n",
    "                    tracker.append([weights,acc])\n",
    "                    print()\n",
    "    max_acc = 0\n",
    "    tracker = np.asarray(tracker)\n",
    "    for i,sample in enumerate(tracker):\n",
    "    if(max_acc<tracker[i][1]):\n",
    "        max_acc=tracker[i][1]\n",
    "        opt_weight = tracker[1][0]\n",
    "    return opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_weight = findWeights(classifiers,processed_test_data,test_data,test_target,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for MNIST is: 0.9832\n",
      "The Confusion Matrix is: \n",
      "     0     1     2    3    4    5    6     7    8    9\n",
      "0  972     1     0    1    0    0    2     1    3    0\n",
      "1    0  1129     1    2    0    1    0     2    0    0\n",
      "2    2     0  1011    1    1    1    1    11    3    1\n",
      "3    1     0     1  981    0   13    0     8    3    3\n",
      "4    0     0     2    1  962    0    3     2    1   11\n",
      "5    2     0     0    3    0  881    2     1    1    2\n",
      "6    4     2     0    1    2    2  943     0    4    0\n",
      "7    1     1     6    2    0    0    0  1016    2    0\n",
      "8    0     0     2    4    3    2    1     5  953    4\n",
      "9    1     1     0    2    7    5    1     5    3  984\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  99.183673  98.880977\n",
      "1  99.471366  99.559083\n",
      "2  97.965116  98.826979\n",
      "3  97.128713  98.296593\n",
      "4  97.963340  98.666667\n",
      "5  98.766816  97.348066\n",
      "6  98.434238  98.950682\n",
      "7  98.832685  96.669838\n",
      "8  97.843943  97.944502\n",
      "9  97.522299  97.910448\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_test_data,test_data,opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,test_target)\n",
    "print(\"The Accuracy for MNIST is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '../USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_USPS = one_hot_vect((1,USPSTar),np.unique(USPSTar))\n",
    "scaler.fit(USPSMat)\n",
    "processed_USPSDat = scaler.transform(USPSMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 1s 73us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for USPS is: 0.5059252962648132\n",
      "The Confusion Matrix is: \n",
      "     0    1     2     3     4     5     6    7    8    9\n",
      "0  836   71    54    13    32    54   116   48  150   37\n",
      "1    4  575     4     4    71     3    15  234   12  104\n",
      "2   66  144  1548    71    30    58   201  215   90   54\n",
      "3   93  144    86  1465    23    68    44  566  328  259\n",
      "4  186  321    13     3  1002     5    38   31   69  141\n",
      "5  154  131    86   347   129  1647   226   50  362   93\n",
      "6  112   91    79     6    34    15  1201    6   51    6\n",
      "7  150  349    57    21   320    55    40  690   87  659\n",
      "8   90   92    66    61   301    82    69  148  824  317\n",
      "9  309   82     6     9    58    13    50   12   27  330\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  59.248760  41.800000\n",
      "1  56.042885  28.750000\n",
      "2  62.494954  77.438719\n",
      "3  47.626788  73.250000\n",
      "4  55.389718  50.100000\n",
      "5  51.069767  82.350000\n",
      "6  75.015615  60.050000\n",
      "7  28.418451  34.500000\n",
      "8  40.195122  41.200000\n",
      "9  36.830357  16.500000\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_USPSDat,[USPSMat],opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,USPSTar)\n",
    "print(\"The Accuracy for USPS is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemblePlus(classifiers,processed_test_data,test_data):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = softmax(classifiers[0],processed_test_data)\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = classifiers[1].predict(test_data,verbose=True)\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = classifiers[2].predict_proba(processed_test_data)\n",
    "    print(\"Working on SVM Might take a while\")\n",
    "    if(len(np.shape(test_data))==3):\n",
    "        test_data = np.squeeze(test_data)\n",
    "    svm_pred = classifiers[3].predict_proba(test_data)\n",
    "    \n",
    "    featureVec = np.vstack([np.argmax(nn_pred,axis=1),np.argmax(rf_pred,axis=1),np.argmax(svm_pred,axis=1),np.argmax(lr_pred,axis=0)])\n",
    "    \n",
    "    return np.transpose(featureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemblePlusPlus(classifiers,processed_test_data,test_data):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = softmax(classifiers[0],processed_test_data)\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = classifiers[1].predict(test_data,verbose=True)\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = classifiers[2].predict_proba(processed_test_data)\n",
    "    print(\"Working on SVM Might take a while\")\n",
    "    if(len(np.shape(test_data))==3):\n",
    "        test_data = np.squeeze(test_data)\n",
    "    svm_pred = classifiers[3].predict_proba(test_data)\n",
    "    \n",
    "    featureVec = np.vstack([np.argmax(nn_pred,axis=1),np.argmax(rf_pred,axis=1),np.argmax(svm_pred,axis=1),np.argmax(lr_pred,axis=0)])\n",
    "    \n",
    "    return np.transpose(featureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 75us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#train_ensembleOutput = ensemblePlus(classifiers,processed_train_data,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superRF = RandomForestClassifier(n_estimators=5000,verbose=True)\n",
    "superRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 5000 out of 5000 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=True,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superRF.fit(train_ensembleOutput,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 93us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#test_ensembleOutput =  ensemblePlus(classifiers,processed_test_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 2s 87us/step\n",
      "Working on Random Forest\n",
      "Working on SVM Might take a while\n"
     ]
    }
   ],
   "source": [
    "#test_ensembleOutput_USPS = ensemblePlus(classifiers,processed_USPSDat,[USPSMat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 5000 out of 5000 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9812,      0     1     2    3    4    5    6     7    8    9\n",
       " 0  972     1     0    1    0    0    2     1    3    0\n",
       " 1    0  1129     2    2    0    1    0     1    0    0\n",
       " 2    5     1  1007    6    1    3    0     4    4    1\n",
       " 3    1     0     1  988    0    5    0     6    4    5\n",
       " 4    1     0     4    1  957    0    3     3    1   12\n",
       " 5    2     0     0    9    0  874    1     1    2    3\n",
       " 6    5     2     0    1    1    3  941     1    4    0\n",
       " 7    1     7    11    2    0    0    0  1001    4    2\n",
       " 8    0     0     2    5    2    2    1     5  954    3\n",
       " 9    0     0     0    1    8    4    1     1    5  989)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(superRF.predict(test_ensembleOutput),test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 5000 out of 5000 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.49537476873843694,      0    1     2     3    4     5     6    7    8    9\n",
       " 0  848    6    72    95  171   159   107  116   93  333\n",
       " 1   71  577   146   163  299   119    90  356   95   84\n",
       " 2   83    4  1469    85    9   166    62   48   67    6\n",
       " 3   20    4    67  1500    1   296     7   18   65   22\n",
       " 4   33   83    33    24  988   133    33  309  305   59\n",
       " 5   84   10    52    83    1  1594    13   44   88   31\n",
       " 6  152   17   156    37   31   323  1126   34   70   54\n",
       " 7   55  265   216   542   22    76    10  640  160   14\n",
       " 8  156   16    79   304   29   426    49   79  833   29\n",
       " 9   38  140    61   276  123    71     7  568  384  332)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(superRF.predict(test_ensembleOutput_USPS),USPSTar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
