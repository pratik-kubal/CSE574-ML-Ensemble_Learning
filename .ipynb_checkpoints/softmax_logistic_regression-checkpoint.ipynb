{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[tuple_data[1][i]])\n",
    "    return one_hot_encoded\n",
    "\n",
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    # Fix softmax when using batch size 1 the dimension of deno changes\n",
    "    if(len(train_data) == 1):\n",
    "        deno = np.sum(num,axis=0)\n",
    "    else:\n",
    "        deno = np.sum(num,axis=1)\n",
    "        deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def cross_entropy_error(predicted,target):\n",
    "    right = np.log(predicted+ 1/10**30)\n",
    "    loss = np.multiply(np.matrix(target),np.transpose(np.matrix(right)))\n",
    "    return -np.sum(np.sum(loss,axis=1),axis=0)\n",
    "\n",
    "def batch_gradient(predicted,train_data,target):\n",
    "    leftleft = np.transpose(predicted)\n",
    "    left = np.subtract(leftleft,target)\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    gradient = np.dot(left.T,train_withBias)\n",
    "    gradient = gradient/len(train_data)\n",
    "    return np.matrix(gradient,dtype=\"float64\")\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    left = np.array(np.argmax(predicted,axis=0)).flatten()\n",
    "    right = np.argmax(target,axis=1)\n",
    "    confusion_mat = np.zeros((len(np.unique(right)),len(np.unique(left))))\n",
    "    for i in range(len(target)):\n",
    "        if(left[i] == right[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[left[i]][right[i]] =confusion_mat[left[i]][right[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def more_metrics(conf_mat):\n",
    "    true_positives = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        true_positives += conf_mat.iloc[i,i]\n",
    "    conf_mat = np.matrix(conf_mat)\n",
    "    tp_fp = np.array(np.sum(conf_mat,axis=1)).ravel()\n",
    "    relevant_elements = np.array(np.sum(conf_mat,axis=0)).ravel()\n",
    "    for i in range(len(conf_mat)):\n",
    "        precision.append(conf_mat[i,i]/tp_fp[i])\n",
    "        recall.append(conf_mat[i,i]/relevant_elements[i])\n",
    "    return true_positives,precision,recall\n",
    "\n",
    "def epoch_shuffle(processed_train_data,target):\n",
    "    # Merge\n",
    "    trainDF = pd.DataFrame(processed_train_data)\n",
    "    targetDF = pd.DataFrame(target)\n",
    "    result = pd.concat([trainDF.reset_index(),targetDF.reset_index()],join='inner',axis=1)\n",
    "    result = result.sample(frac=1)\n",
    "    \n",
    "    return np.asarray(result.iloc[:,1:np.shape(trainDF)[1]+1]),np.asarray(result.iloc[:,np.shape(trainDF)[1]+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Var\n",
    "classes = np.unique(training_data[1])\n",
    "weights = np.random.randn(len(classes), np.shape(training_data[0])[1]+1) * 0.001\n",
    "train_data = training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = one_hot_vect(training_data,classes)\n",
    "target_val = one_hot_vect(validation_data,classes)\n",
    "target_test = one_hot_vect(test_data,classes)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "scaler.fit(validation_data[0])\n",
    "scaler.fit(test_data[0])\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "processed_val_data = scaler.transform(validation_data[0])\n",
    "processed_test_data = scaler.transform(test_data[0])\n",
    "predicted = softmax(weights,processed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Done: 0.01024\n",
      "Cost of Batch: 3191.8758388915717\n",
      "Percentage Done: 0.02048\n",
      "Cost of Batch: 3189.7329937908717\n",
      "Percentage Done: 0.03072\n",
      "Cost of Batch: 3216.730936190191\n",
      "Percentage Done: 0.04096\n",
      "Cost of Batch: 3187.905868602589\n",
      "Percentage Done: 0.0512\n",
      "Cost of Batch: 3193.2544285841504\n",
      "Percentage Done: 0.06144\n",
      "Cost of Batch: 3207.8128106787976\n",
      "Percentage Done: 0.07168\n",
      "Cost of Batch: 3183.6146998499603\n",
      "Percentage Done: 0.08192\n",
      "Cost of Batch: 3184.2066259615417\n",
      "Percentage Done: 0.09216\n",
      "Cost of Batch: 3181.3453743648624\n",
      "Percentage Done: 0.1024\n",
      "Cost of Batch: 3181.297754155782\n",
      "Percentage Done: 0.11264\n",
      "Cost of Batch: 3178.1537108474968\n",
      "Percentage Done: 0.12288\n",
      "Cost of Batch: 3178.116291463516\n",
      "Percentage Done: 0.13312\n",
      "Cost of Batch: 3175.696013113741\n",
      "Percentage Done: 0.14336\n",
      "Cost of Batch: 3174.9520248395506\n",
      "Percentage Done: 0.1536\n",
      "Cost of Batch: 3172.84614041246\n",
      "Percentage Done: 0.16384\n",
      "Cost of Batch: 3175.2320345425405\n",
      "Percentage Done: 0.17408\n",
      "Cost of Batch: 3172.3612846517926\n",
      "Percentage Done: 0.18432\n",
      "Cost of Batch: 3168.9702486024944\n",
      "Percentage Done: 0.19456\n",
      "Cost of Batch: 3168.3770650073448\n",
      "Percentage Done: 0.2048\n",
      "Cost of Batch: 3168.8929166759744\n",
      "Percentage Done: 0.21504\n",
      "Cost of Batch: 3185.5832927847478\n",
      "Percentage Done: 0.22528\n",
      "Cost of Batch: 3182.058667787656\n",
      "Percentage Done: 0.23552\n",
      "Cost of Batch: 3165.9866290689806\n",
      "Percentage Done: 0.24576\n",
      "Cost of Batch: 3163.534176606414\n",
      "Percentage Done: 0.256\n",
      "Cost of Batch: 3162.1129858213535\n",
      "Percentage Done: 0.26624\n",
      "Cost of Batch: 3159.4835684678696\n",
      "Percentage Done: 0.27648\n",
      "Cost of Batch: 3160.460692599627\n",
      "Percentage Done: 0.28672\n",
      "Cost of Batch: 3157.4515524397675\n",
      "Percentage Done: 0.29696\n",
      "Cost of Batch: 3166.512424149232\n",
      "Percentage Done: 0.3072\n",
      "Cost of Batch: 3176.0379380018735\n",
      "Percentage Done: 0.31744\n",
      "Cost of Batch: 3167.192881945861\n",
      "Percentage Done: 0.32768\n",
      "Cost of Batch: 3155.0771191760755\n",
      "Percentage Done: 0.33792\n",
      "Cost of Batch: 3153.09878645983\n",
      "Percentage Done: 0.34816\n",
      "Cost of Batch: 3154.99690021504\n",
      "Percentage Done: 0.3584\n",
      "Cost of Batch: 3147.425149980706\n",
      "Percentage Done: 0.36864\n",
      "Cost of Batch: 3149.7227558571103\n",
      "Percentage Done: 0.37888\n",
      "Cost of Batch: 3146.7560818229304\n",
      "Percentage Done: 0.38912\n",
      "Cost of Batch: 3159.6898560316235\n",
      "Percentage Done: 0.39936\n",
      "Cost of Batch: 3144.920743395179\n",
      "Percentage Done: 0.4096\n",
      "Cost of Batch: 3143.851174693972\n",
      "Percentage Done: 0.41984\n",
      "Cost of Batch: 3140.618610348967\n",
      "Percentage Done: 0.43008\n",
      "Cost of Batch: 3183.840698645008\n",
      "Percentage Done: 0.44032\n",
      "Cost of Batch: 3139.4175588102844\n",
      "Percentage Done: 0.45056\n",
      "Cost of Batch: 3135.972641652194\n",
      "Percentage Done: 0.4608\n",
      "Cost of Batch: 3136.6689097925228\n",
      "Percentage Done: 0.47104\n",
      "Cost of Batch: 3146.312270834674\n",
      "Percentage Done: 0.48128\n",
      "Cost of Batch: 3132.108529396872\n",
      "Percentage Done: 0.49152\n",
      "Cost of Batch: 3133.528370566907\n",
      "Percentage Done: 0.50176\n",
      "Cost of Batch: 3133.3139658877026\n",
      "Percentage Done: 0.512\n",
      "Cost of Batch: 3131.223852477159\n",
      "Percentage Done: 0.52224\n",
      "Cost of Batch: 3130.1381385477634\n",
      "Percentage Done: 0.53248\n",
      "Cost of Batch: 3127.07976763597\n",
      "Percentage Done: 0.54272\n",
      "Cost of Batch: 3131.405157370974\n",
      "Percentage Done: 0.55296\n",
      "Cost of Batch: 3129.521342841199\n",
      "Percentage Done: 0.5632\n",
      "Cost of Batch: 3127.282766710949\n",
      "Percentage Done: 0.57344\n",
      "Cost of Batch: 3125.6506360449152\n",
      "Percentage Done: 0.58368\n",
      "Cost of Batch: 3118.7896168127263\n",
      "Percentage Done: 0.59392\n",
      "Cost of Batch: 3121.813352035785\n",
      "Percentage Done: 0.60416\n",
      "Cost of Batch: 3202.8229355268877\n",
      "Percentage Done: 0.6144\n",
      "Cost of Batch: 3114.805012320605\n",
      "Percentage Done: 0.62464\n",
      "Cost of Batch: 3120.271082880161\n",
      "Percentage Done: 0.63488\n",
      "Cost of Batch: 3117.9250233140942\n",
      "Percentage Done: 0.64512\n",
      "Cost of Batch: 3115.3779204129796\n",
      "Percentage Done: 0.65536\n",
      "Cost of Batch: 3114.229516764719\n",
      "Percentage Done: 0.6656\n",
      "Cost of Batch: 3115.755737303377\n",
      "Percentage Done: 0.67584\n",
      "Cost of Batch: 3116.729554270132\n",
      "Percentage Done: 0.68608\n",
      "Cost of Batch: 3112.111595406679\n",
      "Percentage Done: 0.69632\n",
      "Cost of Batch: 3111.102128615467\n",
      "Percentage Done: 0.70656\n",
      "Cost of Batch: 3107.409571101677\n",
      "Percentage Done: 0.7168\n",
      "Cost of Batch: 3106.0316529437277\n",
      "Percentage Done: 0.72704\n",
      "Cost of Batch: 3111.9981152985083\n",
      "Percentage Done: 0.73728\n",
      "Cost of Batch: 3105.5895932679086\n",
      "Percentage Done: 0.74752\n",
      "Cost of Batch: 3104.326260278214\n",
      "Percentage Done: 0.75776\n",
      "Cost of Batch: 3099.637986731714\n",
      "Percentage Done: 0.768\n",
      "Cost of Batch: 3099.504665988229\n",
      "Percentage Done: 0.77824\n",
      "Cost of Batch: 3101.8759663008236\n",
      "Percentage Done: 0.78848\n",
      "Cost of Batch: 3104.9384886025564\n",
      "Percentage Done: 0.79872\n",
      "Cost of Batch: 3103.45915413795\n",
      "Percentage Done: 0.80896\n",
      "Cost of Batch: 3101.1938016683716\n",
      "Percentage Done: 0.8192\n",
      "Cost of Batch: 3104.8341086759533\n",
      "Percentage Done: 0.82944\n",
      "Cost of Batch: 3092.6473850472994\n",
      "Percentage Done: 0.83968\n",
      "Cost of Batch: 3091.944229634587\n",
      "Percentage Done: 0.84992\n",
      "Cost of Batch: 3094.881479262962\n",
      "Percentage Done: 0.86016\n",
      "Cost of Batch: 3096.011154672366\n",
      "Percentage Done: 0.8704\n",
      "Cost of Batch: 3093.6503966712417\n",
      "Percentage Done: 0.88064\n",
      "Cost of Batch: 3087.6544026136635\n",
      "Percentage Done: 0.89088\n",
      "Cost of Batch: 3089.460397488276\n",
      "Percentage Done: 0.90112\n",
      "Cost of Batch: 3085.223223517717\n",
      "Percentage Done: 0.91136\n",
      "Cost of Batch: 3083.536762968245\n",
      "Percentage Done: 0.9216\n",
      "Cost of Batch: 3082.493289695244\n",
      "Percentage Done: 0.93184\n",
      "Cost of Batch: 3086.3774171022424\n",
      "Percentage Done: 0.94208\n",
      "Cost of Batch: 3079.515939634816\n",
      "Percentage Done: 0.95232\n",
      "Cost of Batch: 3082.2857362534864\n",
      "Percentage Done: 0.96256\n",
      "Cost of Batch: 3077.1928378557755\n",
      "Percentage Done: 0.9728\n",
      "Cost of Batch: 3079.9939224532563\n",
      "Percentage Done: 0.98304\n",
      "Cost of Batch: 3079.59912586306\n",
      "Percentage Done: 0.99328\n",
      "Cost of Batch: 1883.4002252504947\n"
     ]
    }
   ],
   "source": [
    "batchSize = 512\n",
    "start = 0\n",
    "end = batchSize\n",
    "batches = len(processed_train_data)/batchSize\n",
    "weights = np.random.randn(len(classes), np.shape(training_data[0])[1]+1) * 0.001\n",
    "predicted = softmax(weights,processed_train_data[start:end,:])\n",
    "cost_initial = np.asscalar(cross_entropy_error(softmax(weights,processed_train_data),target))\n",
    "cost_new = cost_initial*10\n",
    "training_loss =[]\n",
    "val_loss=[]\n",
    "i=1\n",
    "logging=False\n",
    "while(i<batches):\n",
    "        print(\"Percentage Done: \"+str(i/batches))\n",
    "        new_weights = batch_gradient(predicted,processed_train_data[start:end,:],target[start:end,:])\n",
    "        weights = weights - 0.0003*new_weights\n",
    "        if(logging and i%1==0):\n",
    "            # Predict Train & Validation for these weights\n",
    "            predict_val = softmax(weights,processed_val_data)\n",
    "            # Calculate cost of predicting Validation and Training, To see if the hyper param are working\n",
    "            acc,_  = accuracy(predict_val,target_val)\n",
    "            val_loss.append(acc)\n",
    "            predict_train = softmax(weights,processed_train_data)\n",
    "            acc,_  = accuracy(predict_train,target)\n",
    "            training_loss.append(acc)\n",
    "        #print(\"Training Loss: \"+str(cost))\n",
    "        start = start + batchSize\n",
    "        end = end + batchSize\n",
    "        # New Prediction for the data\n",
    "        predicted = softmax(weights,processed_train_data[start:end,:])\n",
    "        if(not logging):\n",
    "            cost = np.asscalar(cross_entropy_error(predicted,target[start:end,:]))\n",
    "            print(\"Cost of Batch: \"+str(cost))\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of Batch: 447774.5214433932\n",
      "Val Acc: 0.9161\n",
      "Train Acc: 0.90656\n",
      "Cost of Batch: 444569.00375248585\n",
      "Val Acc: 0.9192\n",
      "Train Acc: 0.9149\n",
      "Cost of Batch: 443695.60562662286\n",
      "Val Acc: 0.9223\n",
      "Train Acc: 0.91916\n",
      "Cost of Batch: 443898.7765061805\n",
      "Val Acc: 0.9242\n",
      "Train Acc: 0.92132\n",
      "Cost of Batch: 442905.73542048794\n",
      "Val Acc: 0.9249\n",
      "Train Acc: 0.92288\n",
      "Cost of Batch: 442416.99567592907\n",
      "Val Acc: 0.9252\n",
      "Train Acc: 0.92366\n",
      "Cost of Batch: 442133.4571315744\n",
      "Val Acc: 0.9264\n",
      "Train Acc: 0.925\n",
      "Cost of Batch: 442312.30546869314\n",
      "Val Acc: 0.9251\n",
      "Train Acc: 0.9265\n",
      "Cost of Batch: 441564.2105875264\n",
      "Val Acc: 0.9268\n",
      "Train Acc: 0.9269\n",
      "Cost of Batch: 442136.2830677732\n",
      "Val Acc: 0.9266\n",
      "Train Acc: 0.92836\n",
      "Cost of Batch: 441555.1654576626\n",
      "Val Acc: 0.9266\n",
      "Train Acc: 0.9287\n",
      "Cost of Batch: 442098.0657845792\n",
      "Val Acc: 0.9271\n",
      "Train Acc: 0.92906\n",
      "Cost of Batch: 442150.7130962244\n",
      "Val Acc: 0.9264\n",
      "Train Acc: 0.92926\n",
      "Cost of Batch: 441485.9173708781\n",
      "Val Acc: 0.9279\n",
      "Train Acc: 0.9299\n",
      "Cost of Batch: 441034.51911005325\n",
      "Val Acc: 0.9288\n",
      "Train Acc: 0.93104\n",
      "Cost of Batch: 441161.89215407614\n",
      "Val Acc: 0.9275\n",
      "Train Acc: 0.93124\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "batchSize = 1\n",
    "start = 0\n",
    "end = batchSize\n",
    "batches = len(processed_train_data)/batchSize\n",
    "weights = np.random.randn(len(classes), np.shape(training_data[0])[1]+1) * 0.001\n",
    "predicted = softmax(weights,processed_train_data[start:end,:])\n",
    "cost_initial = np.asscalar(cross_entropy_error(softmax(weights,processed_train_data),target))\n",
    "cost_new = cost_initial*0.1\n",
    "train_acc = 0\n",
    "while(cost_initial-cost_new > 0.001):\n",
    "    i = 0\n",
    "    start = 0\n",
    "    end = batchSize\n",
    "    while(i<batches-1):\n",
    "        #print(\"Percentage Done: \"+str(i/batches))\n",
    "        new_weights = batch_gradient(predicted,processed_train_data[start:end,:],target[start:end,:])\n",
    "        weights = weights - 0.0003*new_weights\n",
    "        if(logging and i%1==0):\n",
    "            # Predict Train & Validation for these weights\n",
    "            predict_val = softmax(weights,processed_val_data)\n",
    "            # Calculate cost of predicting Validation and Training, To see if the hyper param are working\n",
    "            acc,_  = accuracy(predict_val,target_val)\n",
    "            val_loss.append(acc)\n",
    "            predict_train = softmax(weights,processed_train_data)\n",
    "            acc,_  = accuracy(predict_train,target)\n",
    "            training_loss.append(acc)\n",
    "        #print(\"Training Loss: \"+str(cost))\n",
    "        start = start + batchSize\n",
    "        end = end + batchSize\n",
    "        # New Prediction for the data\n",
    "        predicted = softmax(weights,processed_train_data[start:end,:])\n",
    "        i = i+1\n",
    "    cost_initital = cost_new\n",
    "    pred_train = softmax(weights,processed_train_data)\n",
    "    cost_new = np.asscalar(cross_entropy_error(pred_train,target))\n",
    "    predict_val = softmax(weights,processed_val_data)\n",
    "    val_acc,_ = accuracy(predict_val,target_val)\n",
    "    train_acc_old = train_acc\n",
    "    train_acc,_ = accuracy(pred_train,target)\n",
    "    if(float(train_acc) - float(train_acc_old) < 0.000001 ):\n",
    "        # Early stopping where just cost is changing but accuracy is stuck\n",
    "        break\n",
    "    print(\"Cost of Batch: \"+str(cost_new))\n",
    "    print(\"Val Acc: \"+str(val_acc))\n",
    "    print(\"Train Acc: \"+str(train_acc))\n",
    "    processed_train_data,target = epoch_shuffle(processed_train_data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Empty 'DataFrame': no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7cb0196246b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticklabel_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museOffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train_loss.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2939\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2941\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1975\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1977\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             raise TypeError('Empty {0!r}: no numeric data to '\n\u001b[0;32m--> 373\u001b[0;31m                             'plot'.format(numeric_data.__class__.__name__))\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Empty 'DataFrame': no numeric data to plot"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(training_loss)\n",
    "ax = df.plot(figsize=(10,15))\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.savefig('./train_loss.png',bbox_inches='tight')\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Empty 'DataFrame': no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9d254a7274d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticklabel_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museOffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./val_loss.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2939\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2941\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1975\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1977\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             raise TypeError('Empty {0!r}: no numeric data to '\n\u001b[0;32m--> 373\u001b[0;31m                             'plot'.format(numeric_data.__class__.__name__))\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Empty 'DataFrame': no numeric data to plot"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(val_loss)\n",
    "ax = df.plot(figsize=(10,15))\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.savefig('./val_loss.png',bbox_inches='tight')\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9276,      0     1    2    3    4    5    6     7    8    9\n",
       " 0  965     0    4    7    2   12    3     6    4    4\n",
       " 1    0  1043   13    2   10    4    3     5   31    5\n",
       " 2    6     4  901   16    2   11    8     6    8    3\n",
       " 3    1     2   14  918    2   30    0     5   22    9\n",
       " 4    3     1   10    1  930    6    6     7    2   28\n",
       " 5    4     5    5   46    0  795    5     0   22    6\n",
       " 6    6     0    7    4    6   26  939     0    7    1\n",
       " 7    4     1   14    5    4    6    1  1033    9   31\n",
       " 8    2     7   18   23    2   20    2     1  884    6\n",
       " 9    0     1    4    8   25    5    0    27   20  868)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_val = softmax(weights,processed_val_data)\n",
    "accuracy(predict_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9244,      0     1    2    3    4    5    6    7    8    9\n",
       " 0  960     0    9    3    2    7   11    2    6   11\n",
       " 1    0  1106    8    1    4    3    4    9   11    6\n",
       " 2    0     3  930   20    7    2    6   22    8    1\n",
       " 3    1     2   12  916    1   31    1    6   17   11\n",
       " 4    1     1    8    3  918    8    7    6   10   29\n",
       " 5    5     1    4   26    0  783   14    1   27    6\n",
       " 6    8     4   13    3    8   17  911    0   13    0\n",
       " 7    2     2   10   11    4    7    2  950   14   25\n",
       " 8    3    16   34   19    6   30    2    2  857    7\n",
       " 9    0     0    4    8   32    4    0   30   11  913)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test = softmax(weights,processed_test_data)\n",
    "accuracy(predict_test,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conf_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-db842106b2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmore_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'conf_mat' is not defined"
     ]
    }
   ],
   "source": [
    "true_positives,precision,recall = more_metrics(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# USPS Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '../USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_USPS = one_hot_vect((1,USPSTar),classes)\n",
    "scaler.fit(USPSMat)\n",
    "processed_USPSDat = scaler.transform(USPSMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3375668783439172,      0    1     2    3    4    5    6    7    8    9\n",
       " 0  523   74   105   81   34  143  118  137  222   33\n",
       " 1   13  350    92   75   96   67   24  236   77  140\n",
       " 2  146  149  1104  140   31  144  366   58   79   79\n",
       " 3   57  238    97  911   23  129   77  385  252  402\n",
       " 4  153  338    35   16  994   43   62   61  135  121\n",
       " 5  135   99   127  399   56  894  299  105  339   31\n",
       " 6  220   46   196   46   69  244  727   36  203   18\n",
       " 7  231  220    29   42  170   74   50  353   84  381\n",
       " 8  129  346   152  204  304  210   61  442  453  353\n",
       " 9  393  140    62   86  223   52  216  187  156  442)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_USPS = softmax(weights,processed_USPSDat)\n",
    "accuracy(predict_USPS,target_USPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5.37503408e-06, 3.69834962e-07, 2.79342750e-06, ...,\n",
       "         1.70547558e-05, 1.76974197e-07, 1.87009545e-04],\n",
       "        [3.37966925e-06, 1.07965933e-07, 3.25091099e-07, ...,\n",
       "         9.24967841e-05, 3.95829284e-07, 7.08793064e-06],\n",
       "        [7.26833205e-06, 5.04313666e-07, 1.78261529e-06, ...,\n",
       "         8.34814537e-05, 4.30276090e-07, 7.55579221e-06],\n",
       "        ...,\n",
       "        [2.62365956e-05, 2.03227730e-06, 2.22502579e-06, ...,\n",
       "         2.48036457e-04, 1.45435313e-04, 1.30467584e-04],\n",
       "        [3.92569968e-06, 1.89821420e-06, 8.30071770e-07, ...,\n",
       "         2.59259738e-04, 1.39723249e-07, 2.70461446e-05],\n",
       "        [7.28843744e-06, 3.17793972e-07, 5.65148371e-07, ...,\n",
       "         1.14518824e-05, 3.49620059e-06, 5.05272703e-06]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
