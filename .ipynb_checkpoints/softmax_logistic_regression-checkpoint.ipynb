{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[target_list[i]])\n",
    "    return one_hot_encoded\n",
    "\n",
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(training_data[0])[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    deno = np.sum(num,axis=1)\n",
    "    deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def cross_entropy_error(predicted,target):\n",
    "    right = np.log(predicted)\n",
    "    loss = np.multiply(np.matrix(target),np.transpose(np.matrix(right)))\n",
    "    return -np.sum(np.sum(loss,axis=1),axis=0)\n",
    "\n",
    "def batch_gradient(predicted,target):\n",
    "    leftleft = np.transpose(predicted)\n",
    "    left = np.subtract(leftleft,target)\n",
    "    bias = np.ones((np.shape(training_data[0])[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    gradient = np.dot(left.T,train_withBias)\n",
    "    return np.matrix(gradient,dtype=\"float64\")\n",
    "\n",
    "def sgd_gradient(predicted,target):\n",
    "    leftleft = np.transpose(predicted)\n",
    "    left = np.subtract(leftleft,target)\n",
    "    bias = np.ones((np.shape(training_data[0])[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    gradient = np.dot(left.T,train_withBias)\n",
    "    return np.matrix(gradient,dtype=\"float64\")\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    left = np.array(np.argmax(predicted,axis=0)).flatten()\n",
    "    right = np.argmax(target,axis=1)\n",
    "    confusion_mat = np.zeros((len(np.unique(right)),len(np.unique(left))))\n",
    "    for i in range(len(target)):\n",
    "        if(left[i] == right[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[left[i]][right[i]] =confusion_mat[left[i]][right[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def more_metrics(conf_mat):\n",
    "    true_positives = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        true_positives += conf_mat.iloc[i,i]\n",
    "    conf_mat = np.matrix(conf_mat)\n",
    "    tp_fp = np.array(np.sum(conf_mat,axis=1)).ravel()\n",
    "    relevant_elements = np.array(np.sum(conf_mat,axis=0)).ravel()\n",
    "    for i in range(len(conf_mat)):\n",
    "        precision.append(conf_mat[i,i]/tp_fp[i])\n",
    "        recall.append(conf_mat[i,i]/relevant_elements[i])\n",
    "    return true_positives,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Var\n",
    "classes = np.unique(training_data[1])\n",
    "weights = np.random.randn(len(classes), np.shape(training_data[0])[1]+1) * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540976.0224725967"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = one_hot_vect(training_data,classes)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "\n",
    "# -540929.479912691 Initial-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538953.8469144147\n",
      "536918.9606629665\n",
      "534938.2319579476\n",
      "533009.0592003565\n",
      "531129.6796080701\n",
      "529298.4962249042\n",
      "527514.1891057533\n",
      "525775.4442822334\n",
      "524081.02618871175\n",
      "522429.7674005875\n",
      "520820.57040731644\n",
      "519252.39689343044\n",
      "517724.2154190202\n",
      "516235.04346925934\n",
      "514783.92840347404\n",
      "513369.9473793323\n",
      "511992.21467241406\n",
      "510649.871839129\n",
      "509342.0906079751\n",
      "508068.06015401654\n",
      "506826.993138233\n",
      "505618.1135976995\n",
      "504440.68235187454\n",
      "503293.94081014115\n",
      "502177.20372857753\n",
      "501089.78687093745\n",
      "500031.00152124686\n",
      "499000.1978125571\n",
      "497996.7239492451\n",
      "497019.93550446414\n",
      "496069.2214215338\n",
      "495143.9690727727\n",
      "494243.5866933285\n",
      "493367.513862431\n",
      "492515.1821477375\n",
      "491686.03862437856\n",
      "490879.5353145892\n",
      "490095.1373946265\n",
      "489332.330797081\n",
      "488590.61463392916\n",
      "487869.4894716072\n",
      "487168.4651932763\n",
      "486487.07501842163\n",
      "485824.8505043809\n",
      "485181.33732829586\n",
      "484556.0941462795\n",
      "483948.6858806375\n",
      "483358.69354577287\n",
      "482785.6977306854\n",
      "482229.29607132537\n",
      "481689.09381056984\n",
      "481164.70712756185\n",
      "480655.7579304982\n",
      "480161.8805740652\n",
      "479682.7168957117\n",
      "479217.9146021306\n",
      "478767.12892898615\n",
      "478330.02285812434\n",
      "477906.2700572938\n",
      "477495.55024903687\n",
      "477097.55522096483\n",
      "476711.9789026402\n",
      "476338.5231680318\n",
      "475976.90135928226\n",
      "475626.81765259115\n",
      "475288.0105250575\n",
      "474960.2120679892\n",
      "474643.1596588706\n",
      "474336.5989463623\n",
      "474040.2823520035\n",
      "473753.9639706256\n",
      "473477.40901651164\n",
      "473210.38743939647\n",
      "472952.6696015831\n",
      "472704.0398897458\n",
      "472464.2830036816\n",
      "472233.19143454125\n",
      "472010.563116441\n",
      "471796.1978692504\n",
      "471589.9000960493\n",
      "471391.4827683728\n",
      "471200.7651299086\n",
      "471017.56723089435\n",
      "470841.7153726696\n",
      "470673.0376417448\n",
      "470511.36713986495\n",
      "470356.54561131797\n",
      "470208.4128751773\n",
      "470066.8181654304\n",
      "469931.612092108\n",
      "469802.6464617042\n",
      "469679.78181616653\n",
      "469562.87868453155\n",
      "469451.8029547679\n",
      "469346.4225359685\n",
      "469246.6084013005\n",
      "469152.2369541728\n",
      "469063.18714168173\n",
      "468979.339863894\n",
      "468900.5791890703\n",
      "468826.7890706667\n",
      "468757.86223016743\n",
      "468693.69169925275\n",
      "468634.1721496738\n",
      "468579.20089133066\n",
      "468528.6793550693\n",
      "468482.51032169187\n",
      "468440.59930479806\n",
      "468402.852876887\n",
      "468369.18203012616\n",
      "468339.4986715026\n",
      "468313.71733865794\n",
      "468291.7544215573\n",
      "468273.52829714725\n",
      "468258.96006748744\n",
      "468247.9715080091\n",
      "468240.4867454346\n",
      "468236.43251240684\n",
      "468235.73684344173\n",
      "468238.33012986474\n",
      "468244.1428066162\n",
      "468253.1080095574\n",
      "468265.1607863842\n",
      "468280.2378466492\n",
      "468298.2772438918\n",
      "468319.2184037269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-568-c5a109f9a947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.0000003\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocessed_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-494-282b800224fe>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(weights, train_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_withBias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_withBias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# High value Fix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    new_weights = batch_gradient(predicted,target)\n",
    "    weights = weights - 0.0000003*new_weights\n",
    "    predicted = softmax(weights,processed_train_data)\n",
    "    cost = np.asscalar(cross_entropy_error(predicted,target))\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,conf_mat  = accuracy(predicted,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives,precision,recall = more_metrics(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38512"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.700718993409227,\n",
       " 0.633575647432985,\n",
       " 0.8794258373205741,\n",
       " 0.7223564393235458,\n",
       " 0.8376198779424586,\n",
       " 0.8367807446408424,\n",
       " 0.8551165146909828,\n",
       " 0.7990696010019681,\n",
       " 0.8604579207920792,\n",
       " 0.7982810920121335]"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.948499594484996,\n",
       " 0.982388164846777,\n",
       " 0.7399355877616747,\n",
       " 0.762007449519702,\n",
       " 0.7909034780819099,\n",
       " 0.493786063027075,\n",
       " 0.8523530599878812,\n",
       " 0.8629951690821256,\n",
       " 0.5743494423791822,\n",
       " 0.6331194867682438]"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
