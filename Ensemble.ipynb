{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(weights,train_data):\n",
    "    bias = np.ones((np.shape(train_data)[0],1))\n",
    "    train_withBias = np.hstack((train_data,bias))\n",
    "    num = np.dot(weights,train_withBias.T)\n",
    "    # High value Fix\n",
    "    # https://houxianxu.github.io/2015/04/23/logistic-softmax-regression/\n",
    "    num = np.subtract(num,np.max(num,axis=0))\n",
    "    num = np.exp(num)\n",
    "    # Fix softmax when using batch size 1 the dimension of deno changes\n",
    "    if(len(train_data) == 1):\n",
    "        deno = np.sum(num,axis=0)\n",
    "    else:\n",
    "        deno = np.sum(num,axis=1)\n",
    "        deno = deno.reshape((10,1))\n",
    "    return np.divide(num,deno)\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    correct = 0\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    for i in range(len(target)):\n",
    "        if(predicted[i] == target[i]):\n",
    "            correct+=1\n",
    "        confusion_mat[target[i]][predicted[i]] =confusion_mat[target[i]][predicted[i]] +1\n",
    "    return correct/len(target),pd.DataFrame(np.matrix(confusion_mat,dtype=\"int32\"))\n",
    "\n",
    "def one_hot_vect(tuple_data,classes):\n",
    "    one_hot_encoded=np.zeros((len(tuple_data[1]),len(classes)))\n",
    "    identity = np.identity(len(classes))\n",
    "    for i in range(len(tuple_data[1])):\n",
    "        one_hot_encoded[i] = np.add(one_hot_encoded[i],identity[tuple_data[1][i]])\n",
    "    return one_hot_encoded\n",
    "\n",
    "def more_metrics(conf_mat):\n",
    "    true_positives = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        true_positives += conf_mat.iloc[i,i]\n",
    "    conf_mat = np.matrix(conf_mat)\n",
    "    tp_fp = np.array(np.sum(conf_mat,axis=1)).ravel()\n",
    "    relevant_elements = np.array(np.sum(conf_mat,axis=0)).ravel()\n",
    "    for i in range(len(conf_mat)):\n",
    "        precision.append(conf_mat[i,i]/tp_fp[i])\n",
    "        recall.append(conf_mat[i,i]/relevant_elements[i])\n",
    "    return true_positives,precision,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('./models/logisticModel', weights)\n",
    "#np.loadtxt('./models/logisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "train_data = np.append(training_data[0],validation_data[0],axis=0)\n",
    "train_target = np.append(training_data[1],validation_data[1])\n",
    "test_target = test_data[1]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "processed_train_data = scaler.transform(train_data)\n",
    "scaler.fit(test_data)\n",
    "processed_test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#rf = joblib.load(\"./models/randomForestModel.joblib\")\n",
    "#lr = np.loadtxt(\"./models/logisticModel\")\n",
    "#nn = joblib.load(\"./models/DNN_lowHidden.joblib\")\n",
    "#svm model pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve order since LR is hardcoded and methods like rf depend on predic_proba :3\n",
    "classifiers = [lr,nn,rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(classifiers,processed_test_data,test_data,weights):\n",
    "    num_classifiers = len(classifiers)\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    lr_pred = np.multiply(softmax(classifiers[0],processed_test_data),weights[0])\n",
    "    print(\"Working on Neural Network\")\n",
    "    nn_pred = np.multiply(classifiers[1].predict(test_data,verbose=True),weights[1])\n",
    "    print(\"Working on Random Forest\")\n",
    "    rf_pred = np.multiply(classifiers[2].predict_proba(processed_test_data),weights[2])\n",
    "    sumProb = (np.transpose(lr_pred)+rf_pred+nn_pred)\n",
    "    wtAvg = np.divide(sumProb,len(classifiers))\n",
    "    return np.argmax(wtAvg,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [1, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [2, 3, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 2] the accuracy is: 0.1007\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 78us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 1, 3] the accuracy is: 0.1008\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 2] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 5s 77us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 2, 3] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 1] the accuracy is: 0.1006\n",
      "\n",
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "60000/60000 [==============================] - 4s 72us/step\n",
      "Working on Random Forest\n",
      "\n",
      "For weights [3, 3, 2] the accuracy is: 0.1006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brute Force to find best weights\n",
    "def findWeights(classifiers,processed_test_data,test_data,test_target,weights)\n",
    "    tracker = []\n",
    "    for weight1 in range(1,4):\n",
    "        for weight2 in range(1,4):\n",
    "            for weight3 in range(1,4):\n",
    "                if(not(weight1 == weight2 == weight3)):\n",
    "                    weights=[weight1,weight2,weight3]\n",
    "                    predicted = ensemble(classifiers,processed_test_data,test_data,weights)\n",
    "                    acc,_ = accuracy(predicted,test_target)\n",
    "                    print()\n",
    "                    print(\"For weights \"+str(weights)+\" the accuracy is: \"+str(acc))\n",
    "                    tracker.append([weights,acc])\n",
    "                    print()\n",
    "    max_acc = 0\n",
    "    tracker = np.asarray(tracker)\n",
    "    for i,sample in enumerate(tracker):\n",
    "    if(max_acc<tracker[i][1]):\n",
    "        max_acc=tracker[i][1]\n",
    "        opt_weight = tracker[1][0]\n",
    "    return opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_weight = findWeights(classifiers,processed_test_data,test_data,test_target,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for MNIST is: 0.9832\n",
      "The Confusion Matrix is: \n",
      "     0     1     2    3    4    5    6     7    8    9\n",
      "0  972     1     0    1    0    0    2     1    3    0\n",
      "1    0  1129     1    2    0    1    0     2    0    0\n",
      "2    2     0  1011    1    1    1    1    11    3    1\n",
      "3    1     0     1  981    0   13    0     8    3    3\n",
      "4    0     0     2    1  962    0    3     2    1   11\n",
      "5    2     0     0    3    0  881    2     1    1    2\n",
      "6    4     2     0    1    2    2  943     0    4    0\n",
      "7    1     1     6    2    0    0    0  1016    2    0\n",
      "8    0     0     2    4    3    2    1     5  953    4\n",
      "9    1     1     0    2    7    5    1     5    3  984\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  99.183673  98.880977\n",
      "1  99.471366  99.559083\n",
      "2  97.965116  98.826979\n",
      "3  97.128713  98.296593\n",
      "4  97.963340  98.666667\n",
      "5  98.766816  97.348066\n",
      "6  98.434238  98.950682\n",
      "7  98.832685  96.669838\n",
      "8  97.843943  97.944502\n",
      "9  97.522299  97.910448\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_test_data,test_data,opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,test_target)\n",
    "print(\"The Accuracy for MNIST is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '../USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_USPS = one_hot_vect((1,USPSTar),np.unique(USPSTar))\n",
    "scaler.fit(USPSMat)\n",
    "processed_USPSDat = scaler.transform(USPSMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Logistic Regression\n",
      "Working on Neural Network\n",
      "19999/19999 [==============================] - 1s 73us/step\n",
      "Working on Random Forest\n",
      "The Accuracy for USPS is: 0.5059252962648132\n",
      "The Confusion Matrix is: \n",
      "     0    1     2     3     4     5     6    7    8    9\n",
      "0  836   71    54    13    32    54   116   48  150   37\n",
      "1    4  575     4     4    71     3    15  234   12  104\n",
      "2   66  144  1548    71    30    58   201  215   90   54\n",
      "3   93  144    86  1465    23    68    44  566  328  259\n",
      "4  186  321    13     3  1002     5    38   31   69  141\n",
      "5  154  131    86   347   129  1647   226   50  362   93\n",
      "6  112   91    79     6    34    15  1201    6   51    6\n",
      "7  150  349    57    21   320    55    40  690   87  659\n",
      "8   90   92    66    61   301    82    69  148  824  317\n",
      "9  309   82     6     9    58    13    50   12   27  330\n",
      "The Precision & Recall is: \n",
      "   Precision     Recall\n",
      "0  59.248760  41.800000\n",
      "1  56.042885  28.750000\n",
      "2  62.494954  77.438719\n",
      "3  47.626788  73.250000\n",
      "4  55.389718  50.100000\n",
      "5  51.069767  82.350000\n",
      "6  75.015615  60.050000\n",
      "7  28.418451  34.500000\n",
      "8  40.195122  41.200000\n",
      "9  36.830357  16.500000\n"
     ]
    }
   ],
   "source": [
    "predicted = ensemble(classifiers,processed_USPSDat,[USPSMat],opt_weight)\n",
    "acc,conf_mat = accuracy(predicted,USPSTar)\n",
    "print(\"The Accuracy for USPS is: \"+str(acc))\n",
    "print(\"The Confusion Matrix is: \")\n",
    "print(pd.DataFrame(conf_mat))\n",
    "_,precision,recall = more_metrics(pd.DataFrame(conf_mat))\n",
    "print(\"The Precision & Recall is: \")\n",
    "df = pd.DataFrame(np.multiply(precision,100))\n",
    "df.columns = [\"Precision\"]\n",
    "df1 = pd.DataFrame(np.multiply(recall,100))\n",
    "df1.columns = [\"Recall\"]\n",
    "print(pd.concat([df,df1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
